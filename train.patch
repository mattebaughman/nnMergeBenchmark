--- train.lua   2016-05-15 12:18:21.000000000 +0000
+++ train1.lua  2017-07-25 18:47:21.687989214 +0000
@@ -27,15 +27,15 @@
 cmd:option('-batchnorm', 0)
 
 -- Optimization options
-cmd:option('-max_epochs', 50)
+cmd:option('-max_epochs', 1)
 cmd:option('-learning_rate', 2e-3)
 cmd:option('-grad_clip', 5)
 cmd:option('-lr_decay_every', 5)
 cmd:option('-lr_decay_factor', 0.5)
 
 -- Output options
-cmd:option('-print_every', 1)
-cmd:option('-checkpoint_every', 1000)
+cmd:option('-print_every', 0)
+cmd:option('-checkpoint_every', 0)
 cmd:option('-checkpoint_name', 'cv/checkpoint')
 
 -- Benchmark options
@@ -79,7 +79,9 @@
 for k, v in pairs(vocab.idx_to_token) do
   idx_to_token[tonumber(k)] = v
 end
-
+k = 1
+while k < 5 do
+k=k+1
 -- Initialize the model and criterion
 local opt_clone = torch.deserialize(torch.serialize(opt))
 opt_clone.idx_to_token = idx_to_token
@@ -245,3 +247,4 @@
     collectgarbage()
   end
 end
+end

